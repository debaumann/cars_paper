{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28943,"status":"ok","timestamp":1729371810843,"user":{"displayName":"dennis baumann","userId":"10171770900033388352"},"user_tz":-120},"id":"j9kyjaLh4mhx","outputId":"e0162363-64bd-4970-9f06-a2473442758b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers==4.40.2\n","  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/138.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m133.1/138.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.2) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.2) (0.24.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.2) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.2) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.2) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.2) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.2) (2.32.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.2) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.2) (0.4.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.2) (4.66.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.2) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.2) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.2) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.2) (2024.8.30)\n","Downloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: transformers\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.44.2\n","    Uninstalling transformers-4.44.2:\n","      Successfully uninstalled transformers-4.44.2\n","Successfully installed transformers-4.40.2\n"]}],"source":["!pip install transformers==4.40.2"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4135,"status":"ok","timestamp":1729371814969,"user":{"displayName":"dennis baumann","userId":"10171770900033388352"},"user_tz":-120},"id":"3KoYXTPPRxtw","outputId":"8d3b07b7-a336-42f4-c47d-ab4eb2e98a53"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.3)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.16.0)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"]}],"source":["!pip install wandb"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23232,"status":"ok","timestamp":1729371872327,"user":{"displayName":"dennis baumann","userId":"10171770900033388352"},"user_tz":-120},"id":"XpriVThj7H-Z","outputId":"3d479d1c-eb3b-48ef-cc98-103ce5a21292"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":13167,"status":"ok","timestamp":1729371885488,"user":{"displayName":"dennis baumann","userId":"10171770900033388352"},"user_tz":-120},"id":"-RqIHPn84TDv"},"outputs":[],"source":["from transformers import ViTImageProcessor, ViTModel\n","from PIL import Image\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import cv2\n","import wandb\n","import os"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7875,"status":"ok","timestamp":1729371893357,"user":{"displayName":"dennis baumann","userId":"10171770900033388352"},"user_tz":-120},"id":"PzNJeNKb6NtN","outputId":"111e4441-daa1-4af3-9ea3-402e38ae2ee3"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}],"source":["!wandb login"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":622,"status":"ok","timestamp":1729371898024,"user":{"displayName":"dennis baumann","userId":"10171770900033388352"},"user_tz":-120},"id":"KFqqiYlZ8TGE"},"outputs":[],"source":["import random\n","seed = 7\n","def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","set_seed(seed)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":379,"status":"ok","timestamp":1729371900132,"user":{"displayName":"dennis baumann","userId":"10171770900033388352"},"user_tz":-120},"id":"K3r0fQHkrudu"},"outputs":[],"source":["h2o_root = '/content/drive/My Drive/vit_3d/'\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":479,"status":"ok","timestamp":1729371902177,"user":{"displayName":"dennis baumann","userId":"10171770900033388352"},"user_tz":-120},"id":"vBBoZmQw63co"},"outputs":[],"source":["\n","def process_images(rgb_images, hand_heatmaps,obj_heatmaps):\n","    img_size =224\n","\n","    images = rgb_images.unsqueeze(0).permute(2, 1, 3, 4, 0).squeeze(-1)  # Reshape to (batch_size * sequence_length, 3, H, W)\n","\n","    interleaved_sequences = torch.zeros((16, 3, 496, 496), device=rgb_images.device)\n","\n","\n","    # Resize grayscale images\n","    crop_size = 360\n","    start = (images.shape[-1]) // 2\n","    #start = (grayscale_images.shape[-1]) // 2\n","    cropped_grayscale = images[:,:, :, int(start - crop_size / 2):int(start + crop_size / 2)]#grayscale_\n","\n","    resized_grayscale = nn.functional.interpolate(cropped_grayscale, size=(img_size, img_size), mode='nearest').squeeze(0)\n","\n","    # Resize heatmaps\n","    crop_size = 720\n","    start = (hand_heatmaps.shape[-1] - crop_size) // 2\n","    cropped_hand_heatmaps = hand_heatmaps[:,:, start:start + crop_size]\n","    cropped_hand_heatmaps =cropped_hand_heatmaps.unsqueeze(1)\n","\n","    resized_hand_heatmaps = nn.functional.interpolate(cropped_hand_heatmaps, size=(img_size, img_size), mode='nearest').squeeze(0)\n","    resized_hand_heatmaps = resized_hand_heatmaps.float() # Add channel dimension\n","    resized_hand_heatmaps = resized_hand_heatmaps.view( 1,8, img_size, img_size).squeeze(0).unsqueeze(1)\n","\n","    cropped_obj_heatmaps =obj_heatmaps[:,:, start:start + crop_size]\n","    cropped_obj_heatmaps =cropped_obj_heatmaps.unsqueeze(1)\n","\n","    resized_obj_heatmaps = nn.functional.interpolate(cropped_obj_heatmaps, size=(img_size, img_size), mode='nearest').squeeze(0)\n","    resized_obj_heatmaps = resized_obj_heatmaps.float() # Add channel dimension\n","    resized_obj_heatmaps = resized_obj_heatmaps.view( 1,8, img_size, img_size).squeeze(0).unsqueeze(1)\n","\n","\n","    #print(resized_grayscale.shape)\n","    return resized_grayscale, resized_hand_heatmaps, resized_obj_heatmaps\n","\n","\n","class TrainData(torch.utils.data.DataLoader):\n","    def __init__(self):\n","        self.data_path = h2o_root + \"seq_8_train/\"\n","        self.img_path = self.data_path + \"frames_train(1)/\"\n","        self.hand_path = self.data_path + \"poses_hand_train/\"\n","        self.obj_poses = self.data_path + \"poses_obj_train/\"\n","        self.num_actions = len(os.listdir(self.hand_path))\n","        self.labels = np.load(h2o_root + \"action_labels_train.npy\")\n","\n","    def __len__(self):\n","        return self.num_actions\n","\n","    def __getitem__(self, idx):\n","        img = np.load(self.img_path + format(idx + 1, '03d') + \".npy\")\n","        hand_heatmap = np.load(self.data_path + \"heatmaps_train/\" + format(idx + 1, '03d') + \".npy\")\n","        obj_heatmap = np.load(self.data_path + \"obj_heatmaps_train/\" + format(idx + 1, '03d') + \".npy\")\n","        mano_pose = np.load(self.data_path +'mano_8_train/' + format(idx, '03d') + \".npy\")\n","        label = self.labels[idx]\n","\n","        img = np.moveaxis(img, -1, 0)\n","        img = torch.from_numpy(img).float()\n","        #hand_heatmap[hand_heatmap > 0] = 255\n","        hand_heatmap =torch.from_numpy(hand_heatmap/255.0).float()\n","        obj_heatmap =torch.from_numpy(obj_heatmap/255.0).float()\n","        img,hand,obj = process_images(img,hand_heatmap,obj_heatmap)\n","        mano_pose = torch.from_numpy(mano_pose[:,4:52]).float()\n","        return img, hand, obj,mano_pose, label\n","\n","\n","class ValData(torch.utils.data.DataLoader):\n","    def __init__(self):\n","        self.data_path = h2o_root + \"seq_8_val/\"\n","        self.img_path = self.data_path+ \"frames_val(1)/\"\n","        self.hand_path = self.data_path + \"poses_hand_val/\"\n","        self.obj_path  = self.data_path + \"poses_obj_val/\"\n","        self.num_actions = len(os.listdir(self.hand_path))\n","        self.labels = np.load(h2o_root + \"action_labels_val.npy\")\n","\n","    def __len__(self):\n","        return self.num_actions\n","\n","    def __getitem__(self, idx):\n","        img = np.load(self.img_path + format(idx + 1, '03d') + \".npy\")\n","        hand_poses = np.load(self.hand_path + format(idx + 1, '03d') + \".npy\")\n","        obj_poses = np.load(self.obj_path + format(idx + 1, '03d') + \".npy\")\n","        hand_heatmap = np.load(self.data_path + \"heatmaps_val/\" + format(idx + 1, '03d') + \".npy\")\n","        obj_heatmap = np.load(self.data_path + \"obj_heatmaps_val/\" + format(idx + 1, '03d') + \".npy\")\n","        mano_pose = np.load(self.data_path +'mano_8_val/' + format(idx, '03d') + \".npy\")\n","        mano_pose = torch.from_numpy(mano_pose[:,4:52]).float()\n","        label = self.labels[idx]\n","\n","\n","        img = np.moveaxis(img, -1, 0)\n","        img = torch.from_numpy(img).float()\n","        #hand_heatmap[hand_heatmap > 0] = 255\n","        hand_heatmap =torch.from_numpy(hand_heatmap/255.0).float()\n","        obj_heatmap =torch.from_numpy(obj_heatmap/255.0).float()\n","        img,hand,obj = process_images(img,hand_heatmap,obj_heatmap)\n","        return img, hand, obj,mano_pose, label\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":335,"status":"ok","timestamp":1729371905953,"user":{"displayName":"dennis baumann","userId":"10171770900033388352"},"user_tz":-120},"id":"zn9YdGCl7uqi"},"outputs":[],"source":["class Cars_Mano_Action(nn.Module):\n","    def __init__(self, vit_model, num_classes, mano_pose_dim, sequence_length):\n","        super(Cars_Mano_Action, self).__init__()\n","        self.vit_model = vit_model\n","        self.sequence_length = sequence_length\n","        self.mano_pose_dim = 48\n","\n","        self.classifier = nn.Sequential(\n","            nn.Linear(vit_model.config.hidden_size * sequence_length, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(512, num_classes)\n","        )\n","        self.mano_pose_predictor = nn.Sequential(\n","            nn.Linear(vit_model.config.hidden_size, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(512, self.mano_pose_dim)\n","        )\n","\n","    def forward(self, pixel_values):\n","        batch_size = pixel_values.shape[0]\n","        sequence_length = self.sequence_length\n","\n","        # Forward pass through ViT\n","        outputs = self.vit_model(pixel_values, output_attentions=True, interpolate_pos_encoding=True)\n","        last_hidden_state = outputs.last_hidden_state[:, 0, :]  # (batch_size * sequence_length, hidden_size)\n","        attentions = outputs.attentions[-1]  # Get the attention maps from the last layer\n","        # Process attentions to match the input image resolution\n","        num_heads = attentions.shape[1]\n","        num_tokens = attentions.shape[-1] - 1\n","        attentions = attentions[:, :, 0, 1:].reshape(batch_size, num_heads, num_tokens)\n","\n","        w_featmap = pixel_values.shape[-2] // self.vit_model.config.patch_size\n","        h_featmap = pixel_values.shape[-1] // self.vit_model.config.patch_size\n","        attentions = attentions.reshape(batch_size, num_heads, w_featmap, h_featmap)\n","        attentions = F.interpolate(attentions, scale_factor=self.vit_model.config.patch_size, mode=\"nearest\")\n","        attentions = attentions.view(batch_size, num_heads, pixel_values.shape[-2], pixel_values.shape[-1])\n","        attentions = (attentions - attentions.min()) / (attentions.max() - attentions.min())\n","        hand_attention = attentions[:,0:10,:,:]\n","        obj_attention = attentions[:,10:12,:,:]\n","        #free_attention = attentions[:,8:12,:,:]\n","        sum_hand = torch.mean(hand_attention, dim=1)\n","        sum_obj = torch.mean(obj_attention, dim=1)\n","        #mean_hand = torch.mean(hand_attention, dim=1)\n","        #mean_obj = torch.mean(obj_attention, dim=1)\n","        #mean_free =torch.mean(free_attention, dim=1)\n","        # Reshape and concatenate embeddings\n","\n","        concatenated_embeddings = last_hidden_state.reshape(batch_size // sequence_length, sequence_length * self.vit_model.config.hidden_size)\n","        mano_embeddings = last_hidden_state.reshape(batch_size, self.vit_model.config.hidden_size)\n","        # Pass through the classifier\n","        logits = self.classifier(concatenated_embeddings)\n","        mano_poses = self.mano_pose_predictor(mano_embeddings)\n","\n","        return logits, mano_poses, sum_hand, sum_obj#mean_hand,mean_obj#, mean_free\n","\n"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":491,"status":"ok","timestamp":1729376891357,"user":{"displayName":"dennis baumann","userId":"10171770900033388352"},"user_tz":-120},"id":"dnZ8Vp1e8Aj0","outputId":"f8f60638-260b-4f31-f52e-30333d2e657a"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]}],"source":["\n","\n","feature_extractor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224', size=224)\n","#feature_extractor = ViTImageProcessor.from_pretrained('facebook/dino-vitb16',size =496)\n","\n","#vit_model = torch.load(h2o_root + 'model.pth')\n","vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224', add_pooling_layer = False)\n","#\n","#vit_model = ViTModel.from_pretrained('facebook/dino-vitb16', add_pooling_layer=False)\n","\n","\n","# Define the action prediction model\n","num_classes = 37 # Change this to the number of action classes in your dataset\n","mano_pose_dim = 48\n","sequence_length = 8\n","model = Cars_Mano_Action(vit_model, num_classes,mano_pose_dim, sequence_length)\n","#if os.path.exists(h2o_root + \"best_both_heat_action_prediction_model.pth\"):\n"," #   print('loading')\n","  #  model.load_state_dict(torch.load(h2o_root + \"best_both_heat_action_prediction_model.pth\"))\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1077,"status":"ok","timestamp":1729376784280,"user":{"displayName":"dennis baumann","userId":"10171770900033388352"},"user_tz":-120},"id":"vircJ6xw8aq4","outputId":"989462e5-ce7d-4fc1-a38f-80fc7fe7f450"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}],"source":["train_dataset = TrainData()\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True,num_workers=4,\n","    worker_init_fn=lambda _: np.random.seed(int(torch.initial_seed()) % (2**32 - 1)),\n","    generator=torch.Generator().manual_seed(seed))\n","val_dataset = ValData()\n","val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False)\n","\n","# Define the optimizer and loss function\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mano_batch = 8\n","n_comps =48\n","mano_layer_left = ManoLayer(mano_root='manopth/mano/models/',use_pca=False, flat_hand_mean=True, ncomps=48, side='left')\n","\n","\n","def compute_mano_loss(pred_mano, target_mano, shape):\n","\n","    mano_keypoints_3d_pred = mano_layer_left(pred_mano,shape)\n","    mano_keypoints_3d_target = mano_layer_left(target_mano,shape)\n","    print('pred_shape',mano_keypoints_3d_pred.shape)\n","    print('targetshape',mano_keypoints_3d_target.shape)\n","\n","    pred_joints = mano_keypoints_3d_pred[1]\n","    target_joints = mano_keypoints_3d_target[1]\n","    return F.mse_loss(pred_joints, target_joints)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":335,"status":"ok","timestamp":1729376900783,"user":{"displayName":"dennis baumann","userId":"10171770900033388352"},"user_tz":-120},"id":"zbnsChis9WTu","outputId":"e9d6e86c-8350-47cf-c44e-b5c0018394f2"},"outputs":[{"data":{"text/plain":["Cars_Mano_Action(\n","  (vit_model): ViTModel(\n","    (embeddings): ViTEmbeddings(\n","      (patch_embeddings): ViTPatchEmbeddings(\n","        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","      )\n","      (dropout): Dropout(p=0.0, inplace=False)\n","    )\n","    (encoder): ViTEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x ViTLayer(\n","          (attention): ViTAttention(\n","            (attention): ViTSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): ViTSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (intermediate): ViTIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ViTOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","  )\n","  (classifier): Sequential(\n","    (0): Linear(in_features=6144, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=512, out_features=37, bias=True)\n","  )\n","  (mano_pose_predictor): Sequential(\n","    (0): Linear(in_features=768, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=512, out_features=48, bias=True)\n","  )\n",")"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":222,"referenced_widgets":["ff2ccb25433a469eb34e6b8644a37771","cd0fdeeacf0f44079e5f198369ad5b77","5d21a798dff54034839b3e33f5a986f7","5bf1482ae4d84f4692b5e4911fb91d13","d46da01b40034f1b8126d85409c99b2f","c9ac1e176613476dbf3b1684240ad878","eaad8ada171e4f9f8fa6777603fe9bd9","a45c099fe5dd42dda4dae95af9dfe933"]},"executionInfo":{"elapsed":4029,"status":"ok","timestamp":1729376908213,"user":{"displayName":"dennis baumann","userId":"10171770900033388352"},"user_tz":-120},"id":"vvF42WNfSQtE","outputId":"95cdfa3e-7492-4554-b9f5-c7653dbbe4a4"},"outputs":[{"data":{"text/html":["Finishing last run (ID:namo623i) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ff2ccb25433a469eb34e6b8644a37771","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.012 MB of 0.012 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">frosty-salad-138</strong> at: <a href='https://wandb.ai/debaumann/3dvision/runs/namo623i' target=\"_blank\">https://wandb.ai/debaumann/3dvision/runs/namo623i</a><br/> View project at: <a href='https://wandb.ai/debaumann/3dvision' target=\"_blank\">https://wandb.ai/debaumann/3dvision</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241019_222725-namo623i/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:namo623i). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.18.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20241019_222824-i8ld5k37</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/debaumann/3dvision/runs/i8ld5k37' target=\"_blank\">toasty-butterfly-139</a></strong> to <a href='https://wandb.ai/debaumann/3dvision' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/debaumann/3dvision' target=\"_blank\">https://wandb.ai/debaumann/3dvision</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/debaumann/3dvision/runs/i8ld5k37' target=\"_blank\">https://wandb.ai/debaumann/3dvision/runs/i8ld5k37</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/debaumann/3dvision/runs/i8ld5k37?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7e25f511e4a0>"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["wandb.init(project=\"3dvision\", entity=\"debaumann\")"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":410,"status":"ok","timestamp":1729376915746,"user":{"displayName":"dennis baumann","userId":"10171770900033388352"},"user_tz":-120},"id":"WznzjBj0ibpM"},"outputs":[],"source":["from tqdm import tqdm"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2312485,"status":"error","timestamp":1729379229838,"user":{"displayName":"dennis baumann","userId":"10171770900033388352"},"user_tz":-120},"id":"C0Si6YWQ9aWQ","outputId":"0da9a9aa-030f-43d7-b738-bcefca57adaa"},"outputs":[{"name":"stderr","output_type":"stream","text":["Training Epoch 1/15: 100%|██████████| 569/569 [03:16<00:00,  2.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/15], Loss: 19.5806 ,class: 3.7118,hand:0.0100,obj:0.0111, mano:0.1543\n"]},{"name":"stderr","output_type":"stream","text":["\rValidation Epoch 1/15:   0%|          | 0/122 [00:00<?, ?it/s]WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Validation Epoch 1/15: 100%|██████████| 122/122 [00:42<00:00,  2.85it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation - Epoch [1/15], Accuracy: 0.1311, Val Loss: 18.7838,val_class_loss:3.4394,val_hand_mse:0.0087 ,val_obj_mse:0.0098, val_mano:0.1496\n","Saved best model with validation loss: 18.7838\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 2/15: 100%|██████████| 569/569 [03:19<00:00,  2.85it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [2/15], Loss: 14.8846 ,class: 3.4635,hand:0.0090,obj:0.0105, mano:0.1102\n"]},{"name":"stderr","output_type":"stream","text":["\rValidation Epoch 2/15:   0%|          | 0/122 [00:00<?, ?it/s]WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Validation Epoch 2/15: 100%|██████████| 122/122 [00:48<00:00,  2.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation - Epoch [2/15], Accuracy: 0.1230, Val Loss: 27.0498,val_class_loss:3.2160,val_hand_mse:0.0091 ,val_obj_mse:0.0094, val_mano:0.2346\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 3/15: 100%|██████████| 569/569 [03:31<00:00,  2.69it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [3/15], Loss: 10.5758 ,class: 3.1774,hand:0.0082,obj:0.0100, mano:0.0702\n"]},{"name":"stderr","output_type":"stream","text":["\rValidation Epoch 3/15:   0%|          | 0/122 [00:00<?, ?it/s]WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Validation Epoch 3/15: 100%|██████████| 122/122 [00:51<00:00,  2.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation - Epoch [3/15], Accuracy: 0.2623, Val Loss: 21.5172,val_class_loss:2.8791,val_hand_mse:0.0075 ,val_obj_mse:0.0089, val_mano:0.1829\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 4/15: 100%|██████████| 569/569 [03:29<00:00,  2.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [4/15], Loss: 8.2729 ,class: 2.7439,hand:0.0075,obj:0.0098, mano:0.0516\n"]},{"name":"stderr","output_type":"stream","text":["\rValidation Epoch 4/15:   0%|          | 0/122 [00:00<?, ?it/s]WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Validation Epoch 4/15: 100%|██████████| 122/122 [00:55<00:00,  2.22it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation - Epoch [4/15], Accuracy: 0.3197, Val Loss: 32.2487,val_class_loss:2.2590,val_hand_mse:0.0078 ,val_obj_mse:0.0090, val_mano:0.2964\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 5/15: 100%|██████████| 569/569 [03:21<00:00,  2.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [5/15], Loss: 6.9909 ,class: 2.1623,hand:0.0071,obj:0.0096, mano:0.0447\n"]},{"name":"stderr","output_type":"stream","text":["\rValidation Epoch 5/15:   0%|          | 0/122 [00:00<?, ?it/s]WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Validation Epoch 5/15: 100%|██████████| 122/122 [00:50<00:00,  2.40it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation - Epoch [5/15], Accuracy: 0.3852, Val Loss: 24.6330,val_class_loss:1.7186,val_hand_mse:0.0072 ,val_obj_mse:0.0087, val_mano:0.2258\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 6/15: 100%|██████████| 569/569 [03:24<00:00,  2.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [6/15], Loss: 5.6183 ,class: 1.6313,hand:0.0069,obj:0.0092, mano:0.0364\n"]},{"name":"stderr","output_type":"stream","text":["\rValidation Epoch 6/15:   0%|          | 0/122 [00:00<?, ?it/s]WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Validation Epoch 6/15: 100%|██████████| 122/122 [00:54<00:00,  2.26it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation - Epoch [6/15], Accuracy: 0.5328, Val Loss: 40.6379,val_class_loss:1.3373,val_hand_mse:0.0067 ,val_obj_mse:0.0089, val_mano:0.3897\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 7/15: 100%|██████████| 569/569 [03:28<00:00,  2.73it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [7/15], Loss: 4.4207 ,class: 1.0655,hand:0.0066,obj:0.0090, mano:0.0302\n"]},{"name":"stderr","output_type":"stream","text":["\rValidation Epoch 7/15:   0%|          | 0/122 [00:00<?, ?it/s]WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Validation Epoch 7/15: 100%|██████████| 122/122 [00:52<00:00,  2.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation - Epoch [7/15], Accuracy: 0.6885, Val Loss: 19.7377,val_class_loss:0.9456,val_hand_mse:0.0069 ,val_obj_mse:0.0085, val_mano:0.1847\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 8/15: 100%|██████████| 569/569 [03:30<00:00,  2.70it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [8/15], Loss: 3.9627 ,class: 0.6780,hand:0.0064,obj:0.0088, mano:0.0296\n"]},{"name":"stderr","output_type":"stream","text":["\rValidation Epoch 8/15:   0%|          | 0/122 [00:00<?, ?it/s]WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Validation Epoch 8/15: 100%|██████████| 122/122 [00:49<00:00,  2.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation - Epoch [8/15], Accuracy: 0.6639, Val Loss: 25.9970,val_class_loss:0.8803,val_hand_mse:0.0069 ,val_obj_mse:0.0086, val_mano:0.2479\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 9/15: 100%|██████████| 569/569 [03:28<00:00,  2.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [9/15], Loss: 4.9079 ,class: 0.7417,hand:0.0072,obj:0.0094, mano:0.0381\n"]},{"name":"stderr","output_type":"stream","text":["\rValidation Epoch 9/15:   0%|          | 0/122 [00:00<?, ?it/s]WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Validation Epoch 9/15: 100%|██████████| 122/122 [00:48<00:00,  2.54it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation - Epoch [9/15], Accuracy: 0.7705, Val Loss: 20.6520,val_class_loss:0.7141,val_hand_mse:0.0077 ,val_obj_mse:0.0089, val_mano:0.1959\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 10/15:   2%|▏         | 11/569 [00:07<06:06,  1.52it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-cbe8f549841f>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    770\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\n","img_size = 224\n","# Define the optimizer and loss functionf\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","image_save_dir =  os.path.join(h2o_root, 'evo_new_new_496/')\n","os.makedirs(image_save_dir, exist_ok=True)\n","# Initialize variables for best model saving\n","best_val_loss = float('inf')\n","save_model_path = h2o_root + 'models__new496/'\n","os.makedirs(save_model_path, exist_ok=True)\n","\n","# Training and validation loop\n","num_epochs = 15\n","alpha = 1.0  # Initial weight for classification loss\n","beta = 30.0   # Initial weight for heatmap loss\n","gamma = 10.0\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    running_class_loss =0.0\n","    running_hand_mse = 0.0\n","    running_obj_mse = 0.0\n","    train_loss_mano = 0.0\n","\n","    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\"):\n","\n","        optimizer.zero_grad()\n","        imgs,hand_heatmap, obj_heatmap,mano_pose,label = batch\n","        imgs =imgs.squeeze(0)\n","        hand_heatmap = hand_heatmap.squeeze(0)\n","        obj_heatmap = obj_heatmap.squeeze(0)\n","        mano_pose = mano_pose.squeeze(0)\n","        mano_pose = mano_pose.squeeze(1).to(device)\n","        pixel_values = feature_extractor(images=imgs, return_tensors=\"pt\").pixel_values\n","        pixel_values =  pixel_values.to(device)#pixel_values.to(device)\n","        label = label.to(device)\n","\n","        # Forward pass\n","        logits,mano_pose_pred, hand_attention, obj_attention = model(pixel_values)#,free_attention\n","        loss_class = criterion(logits, label)\n","        loss_manopose = F.mse_loss(mano_pose_pred,mano_pose)\n","        train_loss_mano += loss_manopose.item()\n","        running_class_loss += loss_class.item()\n","\n","        hand_heatmap = hand_heatmap.squeeze(1).to(device)\n","        obj_heatmap = obj_heatmap.squeeze(1).to(device)\n","\n","        #heatmap stuff\n","        obj_mse = F.mse_loss(obj_attention, obj_heatmap)\n","        hand_mse = F.mse_loss(hand_attention,hand_heatmap)\n","        running_hand_mse += obj_mse.item()\n","        running_obj_mse += hand_mse.item()\n","\n","        loss =  alpha * loss_class + beta* hand_mse + gamma * obj_mse + loss_manopose * 100.0\n","\n","        running_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","\n","\n","\n","    avg_train_loss = running_loss / len(train_loader)\n","    avg_class_loss = running_class_loss / len(train_loader)\n","    avg_hand_mse = running_hand_mse / len(train_loader)\n","    avg_obj_mse = running_obj_mse / len(train_loader)\n","    avg_train_mano = train_loss_mano / len(train_loader)\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_train_loss:.4f} ,class: {avg_class_loss:.4f},hand:{avg_hand_mse:.4f},obj:{avg_obj_mse:.4f}, mano:{avg_train_mano:.4f}\")\n","\n","    # Log training loss to wandb\n","    wandb.log({\"train_loss\": avg_train_loss, \"class_loss\": avg_class_loss,\"hand_mse\":avg_hand_mse, \"obj_mse\": avg_obj_mse,\"avg_train_mano\": avg_train_mano, \"epoch\": epoch + 1})\n","\n","    # Validation loop\n","    model.eval()\n","    correct_predictions = 0\n","    total_predictions = 0\n","    val_loss = 0.0\n","    val_class_loss = 0.0\n","    val_hand_mse = 0.0\n","    val_loss_mano = 0.0\n","    val_obj_mse = 0.0\n","\n","\n","    with torch.no_grad():\n","        for batch_idx, batch in enumerate(tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{num_epochs}\")):\n","            imgs,hand_heatmap, obj_heatmap,mano_pose,label = batch\n","            imgs =imgs.squeeze(0)\n","            hand_heatmap = hand_heatmap.squeeze(0)\n","            obj_heatmap = obj_heatmap.squeeze(0)\n","            pixel_values = feature_extractor(images=imgs, return_tensors=\"pt\").pixel_values\n","            pixel_values = pixel_values.to(device)\n","            mano_pose = mano_pose.squeeze(0)\n","            mano_pose = mano_pose.squeeze(1).to(device)\n","            label = label.to(device)\n","\n","\n","            # Forward pass\n","            logits, mano_pose_val,hand_attention, obj_attention = model(pixel_values)#,free_attention\n","            loss_class = criterion(logits, label)\n","            val_class_loss += loss_class.item()\n","            loss_mano_val = F.mse_loss(mano_pose_pred,mano_pose)\n","            val_loss_mano += loss_mano_val.item()\n","\n","\n","            hand_heatmap = hand_heatmap.squeeze(1).to(device)\n","            obj_heatmap = obj_heatmap.squeeze(1).to(device)\n","\n","            #heatmap stuff\n","            obj_mse = F.mse_loss(obj_attention, obj_heatmap)\n","            hand_mse = F.mse_loss(hand_attention,hand_heatmap)\n","\n","            val_hand_mse += obj_mse.item()\n","            val_obj_mse += hand_mse.item()\n","            loss =  alpha * loss_class + beta* hand_mse + gamma * obj_mse + loss_mano_val * 100.0\n","            val_loss += loss.item()\n","            # Calculate accuracy\n","            _, predicted = torch.max(logits.data, 1)\n","            total_predictions += label.size(0)\n","            correct_predictions += (predicted == label).sum().item()\n","\n","            # Calculate attention MSE\n","            #attentions = attentions.mean(dim=2).squeeze(1)  # Average over heads and remove singleton dimension\n","\n","\n","            hand_attention = hand_attention.unsqueeze(1)\n","            obj_attention = obj_attention.unsqueeze(1)\n","            #free_attention = free_attention.unsqueeze(1)\n","\n","\n","\n","            # Log images, heatmaps, and attention masks to wandb\n","            if batch_idx in [0]:#,11,15,63,73\n","                save_batch_dir = os.path.join(image_save_dir, f'image_{batch_idx + 1}')\n","                os.makedirs(save_batch_dir, exist_ok=True)\n","                # Store images, heatmaps, and attentions for logging\n","                # Log first eight images\n","                fig, axes = plt.subplots(5, 8, figsize=(20, 10))\n","                for j in range(8):\n","\n","                    img_np = pixel_values[j].permute(1, 2, 0).cpu().numpy()  # Transpose to (height, width, channels)\n","                    img_save = imgs[j].permute(1, 2, 0).cpu().numpy()\n","                    axes[0,j].imshow(img_np)\n","                    axes[0,j].set_title(f'Input Image {j+1}')\n","                    axes[0,j].axis('off')\n","\n","                    heat_np = hand_heatmap[j].cpu().numpy()  # Transpose to (height, width, channels)\n","                    axes[1,j].imshow(heat_np, cmap='hot', interpolation='nearest')\n","                    axes[1,j].set_title(f'hand_heatmap {j+1}')\n","                    axes[1,j].axis('off')\n","\n","                    obj_np = obj_heatmap[j].cpu().numpy()  # Attention map\n","\n","                    axes[2, j].imshow(obj_np, cmap='hot', interpolation='nearest')\n","                    axes[2, j].set_title('obj heat Map')\n","                    axes[2, j].axis('off')\n","\n","                    hand_attention_np = hand_attention[j].squeeze(0).cpu().numpy()  # Attention map\n","\n","                    axes[3, j].imshow(hand_attention_np, cmap='hot', interpolation='nearest')\n","                    axes[3, j].set_title('hand attention')\n","                    axes[3, j].axis('off')\n","\n","                    obj_attention_np = obj_attention[j].squeeze(0).cpu().numpy()  # Attention map\n","\n","                    axes[4, j].imshow(obj_attention_np, cmap='hot', interpolation='nearest')\n","                    axes[4, j].set_title('obj attention')\n","                    axes[4, j].axis('off')\n","                    plt.imsave(os.path.join(save_batch_dir, f'{epoch+1}_original_{j}.png'), img_save / 255)\n","                    plt.imsave(os.path.join(save_batch_dir, f'{epoch+1}_gt_hand_{j}.png'), heat_np, cmap='hot')\n","                    plt.imsave(os.path.join(save_batch_dir, f'{epoch+1}_gt_obj_{j}.png'), obj_np, cmap='hot')\n","                    plt.imsave(os.path.join(save_batch_dir, f'{epoch+1}_hand_{j}.png'), hand_attention_np, cmap='hot')\n","                    plt.imsave(os.path.join(save_batch_dir, f'{epoch+1}_obj_{j}.png'), obj_attention_np, cmap='hot')\n","                    #free_attention_np = free_attention[j].squeeze(0).cpu().numpy()  # Attention map\n","\n","                    #axes[5, j].imshow(free_attention_np, cmap='hot', interpolation='nearest')\n","                    #axes[5, j].set_title('free attention')\n","                    #axes[5, j].axis('off')\n","\n","\n","                action_prediction = predicted[0].item()\n","                ground_truth = label[0].item()\n","                fig.suptitle(f'Action Prediction: {action_prediction}, Ground Truth: {ground_truth}')\n","                wandb.log({\"val_image_{}\": wandb.Image(fig)})\n","                plt.close(fig)\n","\n","    accuracy = correct_predictions / total_predictions\n","    avg_val_class_loss = val_class_loss / len(val_loader)\n","    avg_val_loss = val_loss / len(val_loader)\n","    avg_val_hand_mse = val_hand_mse / len(val_loader)\n","    avg_val_obj_mse = val_obj_mse / len(val_loader)\n","    avg_val_mano = val_loss_mano / len(val_loader)\n","\n","    print(f\"Validation - Epoch [{epoch + 1}/{num_epochs}], Accuracy: {accuracy:.4f}, Val Loss: {avg_val_loss:.4f},val_class_loss:{avg_val_class_loss:.4f},val_hand_mse:{avg_val_hand_mse:.4f} ,val_obj_mse:{avg_val_obj_mse:.4f}, val_mano:{avg_val_mano:.4f}\")\n","\n","    # Log validation metrics to wandb\n","    wandb.log({\"val_accuracy\": accuracy, \"val_loss\": avg_val_loss,\"val_class_loss\": avg_val_class_loss,\"val_hand_mse\": avg_val_hand_mse,\"val_obj_mse\": avg_obj_mse, \"val_mano\": avg_val_mano, \"epoch\": epoch + 1})\n","\n","    # Save the best model based on validation loss\n","    if avg_val_loss < best_val_loss:\n","        best_val_loss = avg_val_loss\n","        best_model_path = save_model_path + f\"{epoch:02d}_mean_dual_10_2_attention_496_1_30_10_action_prediction_model_{avg_val_loss:.2f}.pth\"\n","        torch.save(model.state_dict(), best_model_path)\n","        print(f\"Saved best model with validation loss: {avg_val_loss:.4f}\")\n","\n","# Save the final trained model\n","final_model_path = h2o_root + \"final_concatenated_action_prediction_model.pth\"\n","torch.save(model.state_dict(), final_model_path)\n"]},{"cell_type":"markdown","metadata":{"id":"8AXca0Y8Gs6H"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the path to the model weights\n","weight_path = h2o_root + \"best_both_heat_action_prediction_model.pth\"\n","\n","# Load the model weights\n","model.load_state_dict(torch.load(weight_path))\n","\n","# Move the model to the appropriate device\n","model.to(device)"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48779,"status":"ok","timestamp":1729379332534,"user":{"displayName":"dennis baumann","userId":"10171770900033388352"},"user_tz":-120},"id":"zbhN6Fmj1RRI","outputId":"d79ce760-68f1-4456-ec28-e7aa7a9fdb3b"},"outputs":[{"name":"stderr","output_type":"stream","text":["Validation Epoch 10/15: 100%|██████████| 122/122 [00:48<00:00,  2.52it/s]\n"]}],"source":["save_dir = h2o_root + 'mano_val_out_good/'\n","os.makedirs(save_dir, exist_ok=True)\n","with torch.no_grad():\n","        for batch_idx, batch in enumerate(tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{num_epochs}\")):\n","            imgs,hand_heatmap, obj_heatmap,mano_pose,label = batch\n","            imgs =imgs.squeeze(0)\n","            hand_heatmap = hand_heatmap.squeeze(0)\n","            obj_heatmap = obj_heatmap.squeeze(0)\n","            pixel_values = feature_extractor(images=imgs, return_tensors=\"pt\").pixel_values\n","            pixel_values = pixel_values.to(device)\n","            mano_pose = mano_pose.squeeze(0)\n","            mano_pose = mano_pose.squeeze(1).to(device)\n","            label = label.to(device)\n","\n","\n","            # Forward pass\n","            logits, mano_pose_val,hand_attention, obj_attention = model(pixel_values)#,free_attention\n","            mano_pose_val = mano_pose_val.cpu().numpy()  # Convert to numpy\n","            file_path = os.path.join(save_dir, f\"{batch_idx:03d}.npy\")\n","            np.save(file_path, mano_pose_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X3tuBTMYR5AI"},"outputs":[],"source":["class TestData(torch.utils.data.DataLoader):\n","    def __init__(self):\n","        self.img_path = h2o_root + \"framesequences_8_test/\"\n","        self.num_actions = int(len(os.listdir(self.img_path)))\n","\n","    def __len__(self):\n","        return self.num_actions\n","\n","    def __getitem__(self, idx):\n","        img = np.load(self.img_path + format(idx + 1) + \".npy\")\n","        img = np.moveaxis(img, -1, 0)\n","        img = torch.from_numpy(img).float()\n","\n","        return img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZexBBrZR7Jk"},"outputs":[],"source":["test_dataset = TestData()\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"liNF7NqQSASs"},"outputs":[],"source":["save_model_path = h2o_root + 'models__new496/'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16844,"status":"ok","timestamp":1718667674859,"user":{"displayName":"dennis baumann","userId":"10171770900033388352"},"user_tz":-120},"id":"s2Almcwd2tLX","outputId":"1e228ead-57d1-44f8-c6c5-a0c4cfc6631b"},"outputs":[{"name":"stdout","output_type":"stream","text":["['/content/drive/My Drive/vit_3d/models__new496/00_mean_dual_10_2_attention_496_1_30_10_action_prediction_model_3.81.pth', '/content/drive/My Drive/vit_3d/models__new496/01_mean_dual_10_2_attention_496_1_30_10_action_prediction_model_2.70.pth', '/content/drive/My Drive/vit_3d/models__new496/02_mean_dual_10_2_attention_496_1_30_10_action_prediction_model_1.42.pth', '/content/drive/My Drive/vit_3d/models__new496/03_mean_dual_10_2_attention_496_1_30_10_action_prediction_model_0.90.pth', '/content/drive/My Drive/vit_3d/models__new496/04_mean_dual_10_2_attention_496_1_30_10_action_prediction_model_0.51.pth', '/content/drive/My Drive/vit_3d/models__new496/06_mean_dual_10_2_attention_496_1_30_10_action_prediction_model_0.35.pth', '/content/drive/My Drive/vit_3d/models__new496/07_mean_dual_10_2_attention_496_1_30_10_action_prediction_model_0.28.pth', '/content/drive/My Drive/vit_3d/models__new496/14_mean_dual_10_2_attention_496_1_30_10_action_prediction_model_0.25.pth']\n"]}],"source":["model_files = [os.path.join(save_model_path, f) for f in os.listdir(save_model_path) if f.endswith('.pth')]\n","models = []\n","print(sorted(model_files))\n","for model_file in sorted(model_files):\n","    model = InterleaveHeatmapViTActionPredictionModel(vit_model, num_classes, sequence_length)\n","    model.load_state_dict(torch.load(model_file))\n","    models.append(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WrPs8u4N26gy"},"outputs":[],"source":["import json\n","json_base = save_model_path + 'action_results/'\n","\n","os.makedirs(json_base, exist_ok=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":810429,"status":"ok","timestamp":1718668492101,"user":{"displayName":"dennis baumann","userId":"10171770900033388352"},"user_tz":-120},"id":"MFBjRU_j3j2z","outputId":"31c9d6c9-d403-499f-921b-50a95a60b9d2"},"outputs":[{"name":"stderr","output_type":"stream","text":["testdation Epoch 1/1: 100%|██████████| 242/242 [01:40<00:00,  2.40it/s]\n","testdation Epoch 1/1: 100%|██████████| 242/242 [01:40<00:00,  2.40it/s]\n","testdation Epoch 1/1: 100%|██████████| 242/242 [01:40<00:00,  2.40it/s]\n","testdation Epoch 1/1: 100%|██████████| 242/242 [01:40<00:00,  2.40it/s]\n","testdation Epoch 1/1: 100%|██████████| 242/242 [01:40<00:00,  2.40it/s]\n","testdation Epoch 1/1: 100%|██████████| 242/242 [01:40<00:00,  2.40it/s]\n","testdation Epoch 1/1: 100%|██████████| 242/242 [01:40<00:00,  2.40it/s]\n","testdation Epoch 1/1: 100%|██████████| 242/242 [01:40<00:00,  2.40it/s]\n"]}],"source":["for i,m in enumerate(models):\n","    m.to(device)\n","    m.eval()\n","    epoch =0\n","    num_epochs = 1\n","    img_size =224\n","    predictions ={}\n","    predictions[\"modality\"] = \"training: rgb + heatmaps, test: rgb\"\n","    with torch.no_grad():\n","        for batch_idx, batch in enumerate(tqdm(test_loader, desc=f\"testdation Epoch {epoch + 1}/{num_epochs}\")):\n","            img = batch\n","            images = img.permute(2, 1, 3, 4, 0).squeeze(-1) # Reshape to (batch_size * sequence_length, 3, H, W)\n","            # Define the crop size and preprocess images\n","            crop_size = 360\n","            start = (images.shape[-1]) // 2\n","            cropped_images = images[:, :, :, int(start-crop_size/2):int(start+crop_size/2)]\n","            cropped_images = nn.functional.interpolate(cropped_images, size=(img_size, img_size), mode='nearest')\n","            pixel_values = feature_extractor(images=cropped_images, return_tensors=\"pt\").pixel_values\n","            pixel_values = pixel_values.to(device)\n","\n","            # Forward pass\n","            logits, hand,obj = m(pixel_values)#,obj\n","\n","            # Calculate accuracy\n","            _, predicted = torch.max(logits.data, 1)\n","\n","            # Calculate attention MSE\n","            #attentions = attentions.mean(dim=2).squeeze(1)  # Average over heads and remove singleton dimension\n","            # Store images, heatmaps, and attentions for logging\n","          # Log first eight images\n","\n","            predictions[f'{batch_idx + 1}'] = predicted[0].item()\n","\n","            action_prediction = predicted[0].item()\n","\n","\n","    with open(json_base + f'action_labels{i}.json', 'w') as json_file:\n","        json.dump(predictions, json_file)\n","    # Log validation metrics to wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8046,"status":"ok","timestamp":1718671256367,"user":{"displayName":"dennis baumann","userId":"10171770900033388352"},"user_tz":-120},"id":"pQ_6Y6BO3o06","outputId":"eb41faa8-c549-40c5-e7c8-f8f9596a4aed"},"outputs":[{"name":"stdout","output_type":"stream","text":["Prediction Score: 87.60330578512396%\n","Prediction Score: 87.19008264462809%\n","Prediction Score: 44.62809917355372%\n","Prediction Score: 59.09090909090909%\n","Prediction Score: 69.42148760330579%\n","Prediction Score: 76.03305785123968%\n","Prediction Score: 85.53719008264463%\n","Prediction Score: 87.19008264462809%\n","Prediction Score: 87.60330578512396%\n","Prediction Score: 88.01652892561982%\n"]}],"source":["json_files = [os.path.join(json_base, f) for f in os.listdir(json_base) if f.endswith('.json')]\n","with open(h2o_root + 'action_labels_gt.json', 'r') as file:\n","    gt_data = json.load(file)\n","\n","for i,j in enumerate(json_files):\n","    with open(j, 'r') as file:\n","        data = json.load(file)\n","    data.pop('modality', None)\n","    score = 0\n","    for key in data:\n","        if key in gt_data and data[key] == gt_data[key]:\n","\n","            score += 1\n","\n","    prediction_score = score / len(data) * 100\n","    print(f\"Prediction Score: {prediction_score}%\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"11WyCy19p8GHpSXI3HYBatGEXIfMcWbLv","timestamp":1718545661617},{"file_id":"13v1e4X7Zwddf8cyazTZNYDqP9TOISHtU","timestamp":1718277630155},{"file_id":"1slZdPkQxeI2u8jB6L0FSo3bo8P_ojBKp","timestamp":1718054815381},{"file_id":"1Esvm5_3yrI29dijDpIcVP0DQL22Z5uJS","timestamp":1718041356813},{"file_id":"1A-IHn57_ZLV7JlAYGVIQXAga43oxV-Zu","timestamp":1717935641096},{"file_id":"1W2DCMd0kjrP_8XBjNXr223xJhsMWvBCB","timestamp":1716189353302}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.16"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5bf1482ae4d84f4692b5e4911fb91d13":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d21a798dff54034839b3e33f5a986f7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_eaad8ada171e4f9f8fa6777603fe9bd9","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a45c099fe5dd42dda4dae95af9dfe933","value":1}},"a45c099fe5dd42dda4dae95af9dfe933":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c9ac1e176613476dbf3b1684240ad878":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd0fdeeacf0f44079e5f198369ad5b77":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d46da01b40034f1b8126d85409c99b2f","placeholder":"​","style":"IPY_MODEL_c9ac1e176613476dbf3b1684240ad878","value":"0.012 MB of 0.012 MB uploaded\r"}},"d46da01b40034f1b8126d85409c99b2f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eaad8ada171e4f9f8fa6777603fe9bd9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff2ccb25433a469eb34e6b8644a37771":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_cd0fdeeacf0f44079e5f198369ad5b77","IPY_MODEL_5d21a798dff54034839b3e33f5a986f7"],"layout":"IPY_MODEL_5bf1482ae4d84f4692b5e4911fb91d13"}}}}},"nbformat":4,"nbformat_minor":0}
